{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jndo4G1ieehQ",
        "outputId": "ca2200cb-0485-49a6-9058-82f58a540690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Students are studying machine learning, and they are enjoying coding!\"\n",
        "print(\"Original Text:\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jUKKDt9e2Qq",
        "outputId": "aed0a857-311e-4e27-d639-cfae1be12e19"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Students are studying machine learning, and they are enjoying coding!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) **TOKENIZATION**\n"
      ],
      "metadata": {
        "id": "D-CtietOhsxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Whitespace Tokenizer\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "ws = WhitespaceTokenizer()\n",
        "tokens_ws = ws.tokenize(text)\n",
        "print(\"Whitespace Tokenizer:\")\n",
        "print(tokens_ws)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvlPBpfoe80Y",
        "outputId": "38904483-56c7-4224-db32-3dd22f4d4965"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenizer:\n",
            "['Students', 'are', 'studying', 'machine', 'learning,', 'and', 'they', 'are', 'enjoying', 'coding!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Punctuation-based Tokenizer (using regex)\n",
        "import re\n",
        "\n",
        "tokens_punc = re.split(r'[.,!?\\s]+', text)\n",
        "print(\"Punctuation Tokenizer:\")\n",
        "print(tokens_punc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz6Q_opliDs6",
        "outputId": "f50dd29f-52e7-4290-f022-df336d12d21e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation Tokenizer:\n",
            "['Students', 'are', 'studying', 'machine', 'learning', 'and', 'they', 'are', 'enjoying', 'coding', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c) Treebank Tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens_treebank = word_tokenize(text)\n",
        "print(\"Treebank Tokenizer:\")\n",
        "print(tokens_treebank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZQmh9yviIaS",
        "outputId": "8c18adf7-f7b5-4adf-850c-1ae0cc389586"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokenizer:\n",
            "['Students', 'are', 'studying', 'machine', 'learning', ',', 'and', 'they', 'are', 'enjoying', 'coding', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# d) Tweet Tokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet = TweetTokenizer()\n",
        "tokens_tweet = tweet.tokenize(text)\n",
        "print(\"Tweet Tokenizer:\")\n",
        "print(tokens_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNWA94ajiLkp",
        "outputId": "48069fd1-f028-4ca2-ad94-e1e7739b0e6f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Tokenizer:\n",
            "['Students', 'are', 'studying', 'machine', 'learning', ',', 'and', 'they', 'are', 'enjoying', 'coding', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# e) MWE Tokenizer (natural language as one word)\n",
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe = MWETokenizer([('machine', 'learning')], separator='_')\n",
        "tokens_mwe = mwe.tokenize(word_tokenize(text))\n",
        "print(\"MWE Tokenizer:\")\n",
        "print(tokens_mwe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgiLu636iRBD",
        "outputId": "550afff6-9548-487a-8b17-06d1db7bd68b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MWE Tokenizer:\n",
            "['Students', 'are', 'studying', 'machine_learning', ',', 'and', 'they', 'are', 'enjoying', 'coding', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b_LMTVURibPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEMMING**"
      ],
      "metadata": {
        "id": "HkHXbUtviWwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# a) Porter Stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "words = word_tokenize(text)\n",
        "porter = PorterStemmer()\n",
        "porter_stems = [porter.stem(w) for w in words]\n",
        "print(\"Porter Stemmer:\")\n",
        "print(porter_stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYlvV8AAiWNE",
        "outputId": "babde654-39b6-4c39-967e-5e3945107642"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer:\n",
            "['student', 'are', 'studi', 'machin', 'learn', ',', 'and', 'they', 'are', 'enjoy', 'code', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Snowball Stemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "snowball_stems = [snowball.stem(w) for w in words]\n",
        "print(\"Snowball Stemmer:\")\n",
        "print(snowball_stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jexKUOd3ihGq",
        "outputId": "365d1dbb-3708-468c-c3ed-70d0b00feac8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball Stemmer:\n",
            "['student', 'are', 'studi', 'machin', 'learn', ',', 'and', 'they', 'are', 'enjoy', 'code', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LEMMATIZATION**"
      ],
      "metadata": {
        "id": "gIxCBDfij8j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(w) for w in words]\n",
        "print(\"WordNet Lemmatizer:\")\n",
        "print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie8fE5KijGjL",
        "outputId": "b53f57ca-4a7d-4902-f8d0-159e5508fa72"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordNet Lemmatizer:\n",
            "['Students', 'are', 'studying', 'machine', 'learning', ',', 'and', 'they', 'are', 'enjoying', 'coding', '!']\n"
          ]
        }
      ]
    }
  ]
}